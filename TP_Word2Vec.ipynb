{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNUjIDx+Ytu1AiGhDqfTcsI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/MLB_Mai/blob/main/TP_Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploration et Entraînement d'Embeddings Word2Vec avec Gensim**\n",
        "Dans ce TP, nous allons explorer les embeddings de mots en deux étapes. Dans un premier temps, nous utiliserons un modèle Word2Vec pré-entraîné afin de comprendre comment les relations sémantiques entre les mots sont capturées. Nous analyserons des similarités, des analogies, et visualiserons ces relations dans l'espace vectoriel. Ensuite, dans la seconde partie, nous allons entraîner notre propre modèle Word2Vec sur le corpus Reuters, un ensemble d'articles économiques et financiers. L'objectif est de comprendre le processus d'entraînement d'un modèle Word2Vec et de comparer les relations sémantiques et analogiques obtenues avec un corpus spécifique. À travers ces deux étapes, nous développerons une compréhension pratique des embeddings de mots et de leur utilité dans des tâches de traitement du langage naturel.\n",
        "\n",
        "## Outils :\n",
        "* **Python**\n",
        "* **Gensim** pour Word2Vec\n",
        "* **NLTK** pour le corpus Reuters\n",
        "* **Matplotlib** pour les visualisations\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "g8Un45zScVmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Partie 1 : Explorer un modèle Word2Vec pré-entraîné (Gensim)**\n",
        "Dans cette partie, nous allons utiliser un modèle Word2Vec déjà pré-entraîné pour explorer des similarités et des relations sémantiques entre les mots.\n",
        "\n",
        "**Étape 1** : Charger un modèle Word2Vec pré-entraîné\n",
        "Nous allons utiliser le modèle Google News Word2Vec, qui a été entraîné sur une vaste collection de textes issus de Google News (environ 100 milliards de mots)."
      ],
      "metadata": {
        "id": "REw05tq0cYIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy gensim\n",
        "!pip install gensim --upgrade\n",
        "!pip install numpy==1.23.5 --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "d1f_xAa5VKDY",
        "outputId": "920a78f9-06fa-42e6-a518-d0ad1b6ca8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.23.5\n",
            "Uninstalling numpy-1.23.5:\n",
            "  Successfully uninstalled numpy-1.23.5\n",
            "Found existing installation: gensim 4.3.3\n",
            "Uninstalling gensim-4.3.3:\n",
            "  Successfully uninstalled gensim-4.3.3\n",
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy, gensim\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4\n",
            "Collecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.4.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "65b3d632fa384f1fbe1ddd65920efc4c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Charger le modèle pré-entraîné Google News Word2Vec\n",
        "model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# Vérifier la taille du vocabulaire\n",
        "print(f\"Vocabulaire du modèle : {len(model.key_to_index)} mots\")\n"
      ],
      "metadata": {
        "id": "0K2L7rqXbzIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Étape 2** : Explorer des similarités entre les mots"
      ],
      "metadata": {
        "id": "WAifZcYBdzeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trouver les mots les plus similaires à \"king\"\n",
        "similar_words = model.most_similar('king')\n",
        "print(similar_words)\n"
      ],
      "metadata": {
        "id": "fPslmC3Mb1hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Effectuer des analogies\n",
        "analogy = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "print(analogy)\n"
      ],
      "metadata": {
        "id": "zMxdwgCjb1v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogy = model.most_similar(positive=['woman', 'doctor'], negative=['man'])\n",
        "print(analogy)"
      ],
      "metadata": {
        "id": "Z1gSqBR6cBLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Étape 3** : Visualisation des embeddings"
      ],
      "metadata": {
        "id": "YG14ZrFPd8uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sélectionner des mots pour visualiser leurs embeddings\n",
        "words = ['king', 'queen', 'man', 'woman', 'paris', 'france', 'london', 'england']\n",
        "embeddings = [model[word] for word in words]\n",
        "\n",
        "# Réduction de dimension à 2D avec PCA\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(embeddings)\n",
        "\n",
        "# Visualisation des points\n",
        "plt.scatter(result[:, 0], result[:, 1])\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N7YroeIlcTLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Partie 2 : Entraîner un modèle Word2Vec sur le corpus Reuters**\n",
        "\n",
        "Dans cette partie, vous allez entraîner votre propre modèle Word2Vec en utilisant le corpus Reuters disponible dans NLTK.\n",
        "\n",
        "**Étape 1** : Chargement des données.\n"
      ],
      "metadata": {
        "id": "p__l3LzJebjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "# Télécharger et charger le corpus\n",
        "nltk.download('reuters')\n",
        "\n",
        "# Exemple de texte du corpus\n",
        "corpus = [reuters.raw(fileid) for fileid in reuters.fileids()]\n",
        "\n",
        "# Visualiser un text\n",
        "print(corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06W2nJVlYYcY",
        "outputId": "55cff434-13a4-4990-f0cb-a597d0a338e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\n",
            "  Mounting trade friction between the\n",
            "  U.S. And Japan has raised fears among many of Asia's exporting\n",
            "  nations that the row could inflict far-reaching economic\n",
            "  damage, businessmen and officials said.\n",
            "      They told Reuter correspondents in Asian capitals a U.S.\n",
            "  Move against Japan might boost protectionist sentiment in the\n",
            "  U.S. And lead to curbs on American imports of their products.\n",
            "      But some exporters said that while the conflict would hurt\n",
            "  them in the long-run, in the short-term Tokyo's loss might be\n",
            "  their gain.\n",
            "      The U.S. Has said it will impose 300 mln dlrs of tariffs on\n",
            "  imports of Japanese electronics goods on April 17, in\n",
            "  retaliation for Japan's alleged failure to stick to a pact not\n",
            "  to sell semiconductors on world markets at below cost.\n",
            "      Unofficial Japanese estimates put the impact of the tariffs\n",
            "  at 10 billion dlrs and spokesmen for major electronics firms\n",
            "  said they would virtually halt exports of products hit by the\n",
            "  new taxes.\n",
            "      \"We wouldn't be able to do business,\" said a spokesman for\n",
            "  leading Japanese electronics firm Matsushita Electric\n",
            "  Industrial Co Ltd &lt;MC.T>.\n",
            "      \"If the tariffs remain in place for any length of time\n",
            "  beyond a few months it will mean the complete erosion of\n",
            "  exports (of goods subject to tariffs) to the U.S.,\" said Tom\n",
            "  Murtha, a stock analyst at the Tokyo office of broker &lt;James\n",
            "  Capel and Co>.\n",
            "      In Taiwan, businessmen and officials are also worried.\n",
            "      \"We are aware of the seriousness of the U.S. Threat against\n",
            "  Japan because it serves as a warning to us,\" said a senior\n",
            "  Taiwanese trade official who asked not to be named.\n",
            "      Taiwan had a trade trade surplus of 15.6 billion dlrs last\n",
            "  year, 95 pct of it with the U.S.\n",
            "      The surplus helped swell Taiwan's foreign exchange reserves\n",
            "  to 53 billion dlrs, among the world's largest.\n",
            "      \"We must quickly open our markets, remove trade barriers and\n",
            "  cut import tariffs to allow imports of U.S. Products, if we\n",
            "  want to defuse problems from possible U.S. Retaliation,\" said\n",
            "  Paul Sheen, chairman of textile exporters &lt;Taiwan Safe Group>.\n",
            "      A senior official of South Korea's trade promotion\n",
            "  association said the trade dispute between the U.S. And Japan\n",
            "  might also lead to pressure on South Korea, whose chief exports\n",
            "  are similar to those of Japan.\n",
            "      Last year South Korea had a trade surplus of 7.1 billion\n",
            "  dlrs with the U.S., Up from 4.9 billion dlrs in 1985.\n",
            "      In Malaysia, trade officers and businessmen said tough\n",
            "  curbs against Japan might allow hard-hit producers of\n",
            "  semiconductors in third countries to expand their sales to the\n",
            "  U.S.\n",
            "      In Hong Kong, where newspapers have alleged Japan has been\n",
            "  selling below-cost semiconductors, some electronics\n",
            "  manufacturers share that view. But other businessmen said such\n",
            "  a short-term commercial advantage would be outweighed by\n",
            "  further U.S. Pressure to block imports.\n",
            "      \"That is a very short-term view,\" said Lawrence Mills,\n",
            "  director-general of the Federation of Hong Kong Industry.\n",
            "      \"If the whole purpose is to prevent imports, one day it will\n",
            "  be extended to other sources. Much more serious for Hong Kong\n",
            "  is the disadvantage of action restraining trade,\" he said.\n",
            "      The U.S. Last year was Hong Kong's biggest export market,\n",
            "  accounting for over 30 pct of domestically produced exports.\n",
            "      The Australian government is awaiting the outcome of trade\n",
            "  talks between the U.S. And Japan with interest and concern,\n",
            "  Industry Minister John Button said in Canberra last Friday.\n",
            "      \"This kind of deterioration in trade relations between two\n",
            "  countries which are major trading partners of ours is a very\n",
            "  serious matter,\" Button said.\n",
            "      He said Australia's concerns centred on coal and beef,\n",
            "  Australia's two largest exports to Japan and also significant\n",
            "  U.S. Exports to that country.\n",
            "      Meanwhile U.S.-Japanese diplomatic manoeuvres to solve the\n",
            "  trade stand-off continue.\n",
            "      Japan's ruling Liberal Democratic Party yesterday outlined\n",
            "  a package of economic measures to boost the Japanese economy.\n",
            "      The measures proposed include a large supplementary budget\n",
            "  and record public works spending in the first half of the\n",
            "  financial year.\n",
            "      They also call for stepped-up spending as an emergency\n",
            "  measure to stimulate the economy despite Prime Minister\n",
            "  Yasuhiro Nakasone's avowed fiscal reform program.\n",
            "      Deputy U.S. Trade Representative Michael Smith and Makoto\n",
            "  Kuroda, Japan's deputy minister of International Trade and\n",
            "  Industry (MITI), are due to meet in Washington this week in an\n",
            "  effort to end the dispute.\n",
            "  \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etape 2 : Prétraiter les données."
      ],
      "metadata": {
        "id": "8E3T4EMzfO97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Prétraitement des documents\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "processed_corpus = [preprocess_text(text) for text in corpus]\n",
        "\n",
        "# Compter le nombre total de tokens dans tout le corpus\n",
        "total_tokens = sum(len(tokens) for tokens in processed_corpus)\n",
        "\n",
        "# Afficher le nombre total de tokens\n",
        "print(f\"Nombre total de tokens dans le corpus Reuters : {total_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyQmlVHaYaBH",
        "outputId": "88039515-3012-4224-af5a-e7a2142e59ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de tokens dans le corpus Reuters : 839812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Etape 3** : Entraîner un modèle Word2Vec."
      ],
      "metadata": {
        "id": "wcpJXbPnfba2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Entraînement du modèle Word2Vec\n",
        "model = Word2Vec(processed_corpus, vector_size=100, window=5, min_count=5, sg=0)"
      ],
      "metadata": {
        "id": "zNmwGUuJYgLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Etape 4** : Trouver des analogies sémantiques."
      ],
      "metadata": {
        "id": "7rdyKw8nfkDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = model.wv.most_similar('energy')\n",
        "print(similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czCkx-OIZem4",
        "outputId": "23dc49f3-c630-4991-976a-2a2b3d053777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('petroleum', 0.7597895860671997), ('almir', 0.742469072341919), ('arturo', 0.7198929786682129), ('apea', 0.7046532034873962), ('valero', 0.7012474536895752), ('masse', 0.6994137167930603), ('fernando', 0.6922779083251953), ('hernandez', 0.6842980980873108), ('santos', 0.6823518872261047), ('commerce', 0.6551962494850159)]\n"
          ]
        }
      ]
    }
  ]
}